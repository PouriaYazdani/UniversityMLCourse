{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method for the first layer\n",
    "def calculate_first_layer(X,W,b):\n",
    "    # Define the weight matrix W (3x2)\n",
    "    # W = np.array([\n",
    "    #     [0.4, -0.3],  # w11, w12\n",
    "    #     [0.2,  0.2],  # w21, w22\n",
    "    #     [-0.5,  0.1]   # w31, w32\n",
    "    # ])\n",
    "    \n",
    "    # Define the bias vector (1x2)\n",
    "    # b = np.array([0, -0.4])  # b1, b2\n",
    "    \n",
    "    # Calculate Z = XW + b\n",
    "    Z = np.dot(X, W) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([1,0,0.5])\n",
    "X2 = np.array([0.1, 0.5, 1])\n",
    "X3 = np.array([0,1,0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.array([\n",
    "    [0.4, -0.3],  # w11, w12\n",
    "    [0.2,  0.2],  # w21, w22\n",
    "    [-0.5,  0.1]   # w31, w32\n",
    "])\n",
    "z1 = calculate_first_layer(X1,W0)\n",
    "z2 = calculate_first_layer(X2,W0)\n",
    "z3 = calculate_first_layer(X3,W0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15 -0.65]\n",
      "[-0.36 -0.23]\n",
      "[-0.15 -0.13]\n"
     ]
    }
   ],
   "source": [
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SiLU activation function\n",
    "def silu_activation(Z):\n",
    "    \"\"\"\n",
    "    Applies the SiLU activation function to the input Z.\n",
    "    Z is the output of the first layer (dimension: m x 2).\n",
    "    \"\"\"\n",
    "    # Sigmoid of Z\n",
    "    sigmoid = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # SiLU activation: Z * sigmoid(Z)\n",
    "    silu = Z * sigmoid\n",
    "    return silu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "silu1 = silu_activation(z1)\n",
    "silu2 = silu_activation(z2)\n",
    "silu3 = silu_activation(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08061448 -0.2229432 ]\n",
      "[-0.14794544 -0.10183299]\n",
      "[-0.06938552 -0.06078094]\n"
     ]
    }
   ],
   "source": [
    "print(silu1)\n",
    "print(silu2)\n",
    "print(silu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method for the second layer\n",
    "def calculate_second_layer(Z_activated,W,b):\n",
    "    \"\"\"\n",
    "    Calculates the output of the second layer.\n",
    "    Input:\n",
    "      - Z_activated: Activated output from the first layer (m x 2 matrix)\n",
    "    Output:\n",
    "      - y: Output of the second layer (m x 1 matrix)\n",
    "    \"\"\"\n",
    "    # Weights and bias for the second layer\n",
    "    # W = np.array([0.3, -0.1])  # 2x1 vector\n",
    "    # b = 0.1  # Bias scalar\n",
    "    \n",
    "    # Calculate y = Z_activated * W + b\n",
    "    y = np.dot(Z_activated, W) + b\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "U0 = np.array([0.3, -0.1])  # 2x1 vector\n",
    "\n",
    "f1 = calculate_second_layer(silu1,U0)\n",
    "f2 = calculate_second_layer(silu2,U0)\n",
    "f3 = calculate_second_layer(silu3,U0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14647866296669132\n",
      "0.06579966622256907\n",
      "0.0852624370574336\n"
     ]
    }
   ],
   "source": [
    "print(f1)\n",
    "print(f2)\n",
    "print(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function\n",
    "def sigmoid_activation(y):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid activation function to the input y.\n",
    "    Input:\n",
    "      - y: Output from the second layer (m x 1 matrix)\n",
    "    Output:\n",
    "      - activated_y: Sigmoid-activated output (m x 1 matrix)\n",
    "    \"\"\"\n",
    "    # Apply the sigmoid function\n",
    "    activated_y = 1 / (1 + np.exp(-y))\n",
    "    return activated_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = sigmoid_activation(f1)\n",
    "y2 = sigmoid_activation(f2)\n",
    "y3 = sigmoid_activation(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.536554329964136\n",
      "0.5164439839996734\n",
      "0.5213027055010718\n"
     ]
    }
   ],
   "source": [
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CE loss function with individual losses\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the binary cross-entropy loss.\n",
    "    Inputs:\n",
    "      - y_true: True labels (m x 1 matrix, values 0 or 1)\n",
    "      - y_pred: Predicted probabilities (m x 1 matrix, from sigmoid activation)\n",
    "    Outputs:\n",
    "      - mean_loss: Scalar value representing the average loss over the batch\n",
    "      - individual_losses: List of losses for each sample in the batch\n",
    "    \"\"\"\n",
    "    # Ensure numerical stability by clipping predictions\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculate individual losses\n",
    "    individual_losses = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).flatten()\n",
    "    \n",
    "    # Calculate mean loss\n",
    "    mean_loss = np.mean(individual_losses)\n",
    "    \n",
    "    return mean_loss, individual_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0])\n",
    "y_pred = np.array([y1, y2, y3])\n",
    "loss_iter1,individ_losses = cross_entropy_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6952874681915379\n",
      "0.6225874546310588\n",
      "0.7265881155764734\n",
      "0.7366868343670815\n"
     ]
    }
   ],
   "source": [
    "print(loss_iter1)\n",
    "print(individ_losses[0])\n",
    "print(individ_losses[1])\n",
    "print(individ_losses[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backward pass for calculating gradient of L w.r.t. w_ij\n",
    "def backward_pass_wij(i, j, x, y_true, U, z, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss with respect to w_ij in the first layer.\n",
    "    \n",
    "    Parameters:\n",
    "        i (int): Index of the first layer neuron.\n",
    "        j (int): Index of the input feature.\n",
    "        x (array): Input vector (1D array).\n",
    "        y_true (float): Ground truth label.\n",
    "        U (1D array): Weight vector of the second layer (shape: 2x1).\n",
    "        z (1D array): Pre-activation output of the first layer (shape: 2).\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "\n",
    "    Returns:\n",
    "        float: Gradient of the loss with respect to w_ij.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Compute ∂L/∂y\n",
    "    dL_dy = -y_true / y + (1 - y_true) / (1 - y)\n",
    "    print(f\"dL/dy: {dL_dy}\")\n",
    "\n",
    "    # 2. Compute ∂y/∂f\n",
    "    dy_df = y * (1 - y)\n",
    "    print(f\"dy/df: {dy_df}\")\n",
    "\n",
    "    # 3. Compute ∂f/∂h_i\n",
    "    df_dhi = U[i]\n",
    "    print(f\"df/dh_{i}: {df_dhi}\")\n",
    "\n",
    "    # 4. Compute ∂h_i/∂z_i\n",
    "    sigmoid_zi = 1 / (1 + np.exp(-z[i]))  # Sigmoid of z_i\n",
    "    dh_dzi = sigmoid_zi + z[i] * sigmoid_zi * (1 - sigmoid_zi)\n",
    "    print(f\"dh_{i}/dz_{i}: {dh_dzi}\")\n",
    "\n",
    "    # 5. Compute ∂z_i/∂w_ij\n",
    "    dz_dwij = x[j]\n",
    "    print(f\"dz_{i}/dw_{i}{j}: {dz_dwij}\")\n",
    "\n",
    "    # 6. Chain rule: ∂L/∂w_ij\n",
    "    dL_dwij = dL_dy * dy_df * df_dhi * dh_dzi * dz_dwij\n",
    "    print(f\"Gradient of L w.r.t. w_{i}{j}: {dL_dwij}\")\n",
    "\n",
    "    return dL_dwij\n",
    "\n",
    "def calculate_jacobian_dl_dW(x, y_true, U, z, y):\n",
    "    \"\"\"\n",
    "    Computes the Jacobian matrix of gradients (∂L/∂W) for all weights in W.\n",
    "\n",
    "    Parameters:\n",
    "        x (array): Input vector (1D array of size 3).\n",
    "        y_true (float): Ground truth label (scalar).\n",
    "        U (1D array): Weight vector of the second layer (shape: 2).\n",
    "        z (1D array): Pre-activation output of the first layer (shape: 2).\n",
    "        y (float): Output of the second layer (scalar).\n",
    "\n",
    "    Returns:\n",
    "        jacobian (2D array): Jacobian matrix (2x3) containing ∂L/∂w_ij.\n",
    "    \"\"\"\n",
    "    # Initialize Jacobian matrix with the same shape as W (2x3)\n",
    "    jacobian = np.zeros((2, 3))\n",
    "\n",
    "    # Iterate over all i (neurons) and j (input features)\n",
    "    for i in range(2):  # 2 neurons in the first layer\n",
    "        for j in range(3):  # 3 input features\n",
    "            # Compute the gradient for w_ij using backward_pass_wij\n",
    "            print(f\"$$$w_{i+1}{j+1} iteration\")\n",
    "            gradient = backward_pass_wij(i, j, x, y_true, U, z, y)\n",
    "            jacobian[i, j] = gradient  # Store the gradient in the Jacobian matrix\n",
    "\n",
    "\n",
    "    # Print the Jacobian matrix\n",
    "    print(\"Jacobian Matrix (∂L/∂W):\")\n",
    "    print(jacobian)\n",
    "\n",
    "    return jacobian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$w_11 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.574719696345381\n",
      "dz_0/dw_00: 1.0\n",
      "Gradient of L w.r.t. w_00: -0.07990540642667802\n",
      "$$$w_12 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.574719696345381\n",
      "dz_0/dw_01: 0.0\n",
      "Gradient of L w.r.t. w_01: -0.0\n",
      "$$$w_13 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.574719696345381\n",
      "dz_0/dw_02: 0.5\n",
      "Gradient of L w.r.t. w_02: -0.03995270321333901\n",
      "$$$w_21 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.19651352282931617\n",
      "dz_1/dw_10: 1.0\n",
      "Gradient of L w.r.t. w_10: 0.00910733412587405\n",
      "$$$w_22 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.19651352282931617\n",
      "dz_1/dw_11: 0.0\n",
      "Gradient of L w.r.t. w_11: 0.0\n",
      "$$$w_23 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.19651352282931617\n",
      "dz_1/dw_12: 0.5\n",
      "Gradient of L w.r.t. w_12: 0.004553667062937025\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[-0.07990541 -0.         -0.0399527 ]\n",
      " [ 0.00910733  0.          0.00455367]]\n"
     ]
    }
   ],
   "source": [
    "gradw1 = calculate_jacobian_dl_dW(x=X1, y_true=y_true[0], U=np.array([0.3, -0.1]), z=z1, y=y_pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$w_11 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.3238137175443828\n",
      "dz_0/dw_00: 0.1\n",
      "Gradient of L w.r.t. w_00: 0.00501694939087098\n",
      "$$$w_12 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.3238137175443828\n",
      "dz_0/dw_01: 0.5\n",
      "Gradient of L w.r.t. w_01: 0.0250847469543549\n",
      "$$$w_13 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.3238137175443828\n",
      "dz_0/dw_02: 1.0\n",
      "Gradient of L w.r.t. w_02: 0.0501694939087098\n",
      "$$$w_21 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.38600592827835595\n",
      "dz_1/dw_10: 0.1\n",
      "Gradient of L w.r.t. w_10: -0.0019935043944756636\n",
      "$$$w_22 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.38600592827835595\n",
      "dz_1/dw_11: 0.5\n",
      "Gradient of L w.r.t. w_11: -0.009967521972378319\n",
      "$$$w_23 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.38600592827835595\n",
      "dz_1/dw_12: 1.0\n",
      "Gradient of L w.r.t. w_12: -0.019935043944756637\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[ 0.00501695  0.02508475  0.05016949]\n",
      " [-0.0019935  -0.00996752 -0.01993504]]\n"
     ]
    }
   ],
   "source": [
    "gradw2 = calculate_jacobian_dl_dW(x=X2, y_true=y_true[1], U=np.array([0.3, -0.1]), z=z2, y=y_pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$w_11 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.425280303654619\n",
      "dz_0/dw_00: 0.0\n",
      "Gradient of L w.r.t. w_00: 0.0\n",
      "$$$w_12 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.425280303654619\n",
      "dz_0/dw_01: 1.0\n",
      "Gradient of L w.r.t. w_01: 0.06650993186744109\n",
      "$$$w_13 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.425280303654619\n",
      "dz_0/dw_02: 0.7\n",
      "Gradient of L w.r.t. w_02: 0.04655695230720876\n",
      "$$$w_21 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.43518262027308185\n",
      "dz_1/dw_10: 0.0\n",
      "Gradient of L w.r.t. w_10: -0.0\n",
      "$$$w_22 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.43518262027308185\n",
      "dz_1/dw_11: 1.0\n",
      "Gradient of L w.r.t. w_11: -0.022686187733540322\n",
      "$$$w_23 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.43518262027308185\n",
      "dz_1/dw_12: 0.7\n",
      "Gradient of L w.r.t. w_12: -0.015880331413478225\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[ 0.          0.06650993  0.04655695]\n",
      " [-0.         -0.02268619 -0.01588033]]\n"
     ]
    }
   ],
   "source": [
    "gradw3 = calculate_jacobian_dl_dW(x=X3, y_true=y_true[2], U=np.array([0.3, -0.1]), z=z3, y=y_pred[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00501695  0.02508475  0.05016949]\n",
      " [-0.0019935  -0.00996752 -0.01993504]]\n"
     ]
    }
   ],
   "source": [
    "print(gradw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Matrix:\n",
      "[[-0.02496282  0.03053156  0.01892458]\n",
      " [ 0.00237128 -0.01088457 -0.01042057]]\n"
     ]
    }
   ],
   "source": [
    "stacked = np.stack((gradw1, gradw2, gradw3), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_w_mean = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_w_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_bi(i, y_true, U, z, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss with respect to b_i in the first layer.\n",
    "    \n",
    "    Parameters:\n",
    "        i (int): Index of the first layer neuron.\n",
    "        y_true (float): Ground truth label.\n",
    "        U (1D array): Weight vector of the second layer (shape: 2).\n",
    "        z (1D array): Pre-activation output of the first layer (shape: 2).\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "\n",
    "    Returns:\n",
    "        float: Gradient of the loss with respect to b_i.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Compute ∂L/∂y\n",
    "    dL_dy = -y_true / y + (1 - y_true) / (1 - y)\n",
    "    print(f\"dL/dy: {dL_dy}\")\n",
    "\n",
    "    # 2. Compute ∂y/∂f\n",
    "    dy_df = y * (1 - y)\n",
    "    print(f\"dy/df: {dy_df}\")\n",
    "\n",
    "    # 3. Compute ∂f/∂h_i\n",
    "    df_dhi = U[i]\n",
    "    print(f\"df/dh_{i}: {df_dhi}\")\n",
    "\n",
    "    # 4. Compute ∂h_i/∂z_i\n",
    "    sigmoid_zi = 1 / (1 + np.exp(-z[i]))  # Sigmoid of z_i\n",
    "    dh_dzi = sigmoid_zi + z[i] * sigmoid_zi * (1 - sigmoid_zi)\n",
    "    print(f\"dh_{i}/dz_{i}: {dh_dzi}\")\n",
    "\n",
    "    # 5. Chain rule: ∂L/∂b_i\n",
    "    dL_dbi = dL_dy * dy_df * df_dhi * dh_dzi\n",
    "    print(f\"Gradient of L w.r.t. b_{i}: {dL_dbi}\")\n",
    "\n",
    "    return dL_dbi\n",
    "\n",
    "\n",
    "def calculate_bias_gradients_b_first(y_true, U, z, y):\n",
    "    \"\"\"\n",
    "    Computes the gradients of the loss with respect to the biases (b1, b2).\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (float): Ground truth label.\n",
    "        U (1D array): Weight vector of the second layer (shape: 2).\n",
    "        z (1D array): Pre-activation output of the first layer (shape: 2).\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "\n",
    "    Returns:\n",
    "        gradients (1D array): Gradients of the loss with respect to biases [b1, b2].\n",
    "    \"\"\"\n",
    "    # Initialize gradient vector for biases (2 biases in the first layer)\n",
    "    gradients = np.zeros(2)\n",
    "\n",
    "    # Compute gradient for each bias b_i\n",
    "    for i in range(2):  # 2 neurons in the first layer\n",
    "        print(f\"$$$b{i+1} iteration\")\n",
    "        gradients[i] = backward_pass_bi(i, y_true, U, z, y)\n",
    "\n",
    "    # Print the gradients\n",
    "    print(\"Gradients of the Loss w.r.t. Biases [b1, b2]:\")\n",
    "    print(gradients)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$b1 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.574719696345381\n",
      "Gradient of L w.r.t. b_0: -0.07990540642667802\n",
      "$$$b2 iteration\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.19651352282931617\n",
      "Gradient of L w.r.t. b_1: 0.00910733412587405\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[-0.07990541  0.00910733]\n"
     ]
    }
   ],
   "source": [
    "gradb1_1 = calculate_bias_gradients_b_first(y_true=y_true[0], U=np.array([0.3, -0.1]), z=z1, y=y_pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$b1 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.3238137175443828\n",
      "Gradient of L w.r.t. b_0: 0.0501694939087098\n",
      "$$$b2 iteration\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.38600592827835595\n",
      "Gradient of L w.r.t. b_1: -0.019935043944756637\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[ 0.05016949 -0.01993504]\n"
     ]
    }
   ],
   "source": [
    "gradb1_2 = calculate_bias_gradients_b_first(y_true=y_true[1], U=np.array([0.3, -0.1]), z=z2, y=y_pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$b1 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_0: 0.3\n",
      "dh_0/dz_0: 0.425280303654619\n",
      "Gradient of L w.r.t. b_0: 0.06650993186744109\n",
      "$$$b2 iteration\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "df/dh_1: -0.1\n",
      "dh_1/dz_1: 0.43518262027308185\n",
      "Gradient of L w.r.t. b_1: -0.022686187733540322\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[ 0.06650993 -0.02268619]\n"
     ]
    }
   ],
   "source": [
    "gradb1_3 = calculate_bias_gradients_b_first(y_true=y_true[2], U=np.array([0.3, -0.1]), z=z3, y=y_pred[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Matrix:\n",
      "[ 0.01225801 -0.0111713 ]\n"
     ]
    }
   ],
   "source": [
    "stacked = np.stack((gradb1_1, gradb1_2, gradb1_3), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_b1_mean = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_b1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_U(y_true, y, h):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss with respect to U (second-layer weights).\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (float): Ground truth label.\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "        h (array): Activated output of the first layer (1D array, size: 2).\n",
    "    \n",
    "    Returns:\n",
    "        gradient_U (array): Gradient of the loss with respect to U (size: 2x1).\n",
    "    \"\"\"\n",
    "    # 1. Compute ∂L/∂y\n",
    "    dL_dy = -y_true / y + (1 - y_true) / (1 - y)\n",
    "    print(f\"dL/dy: {dL_dy}\")\n",
    "\n",
    "    # 2. Compute ∂y/∂f\n",
    "    dy_df = y * (1 - y)\n",
    "    print(f\"dy/df: {dy_df}\")\n",
    "\n",
    "    # 3. Chain rule: ∂L/∂U\n",
    "    dL_dU = dL_dy * dy_df * h\n",
    "    print(f\"Gradient of L w.r.t. U: {dL_dU}\")\n",
    "\n",
    "    # Reshape to 2x1 for consistency\n",
    "    return dL_dU.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def calculate_weights_gradients_U(y_true, y, h):\n",
    "    \"\"\"\n",
    "    Computes the gradients of the loss with respect to the weights (u1, u2) of the second layer.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (float): Ground truth label.\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "        h (array): Activated output of the first layer (1D array, size: 2).\n",
    "    \n",
    "    Returns:\n",
    "        gradients_U (array): Gradient of the loss with respect to U (size: 2x1).\n",
    "    \"\"\"\n",
    "    # Compute gradient for second-layer weights\n",
    "    print(\"$$$ Computing Gradients for U (Second Layer Weights)\")\n",
    "    gradients_U = backward_pass_U(y_true, y, h)\n",
    "\n",
    "    # Print the gradients\n",
    "    print(\"Gradients of the Loss w.r.t. U (Second Layer Weights):\")\n",
    "    print(gradients_U)\n",
    "\n",
    "    return gradients_U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "Gradient of L w.r.t. U: [-0.03736043  0.10332206]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.03736043]\n",
      " [ 0.10332206]]\n"
     ]
    }
   ],
   "source": [
    "gradsU_1 = calculate_weights_gradients_U(y_true[0], y_pred[0], h=silu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "Gradient of L w.r.t. U: [-0.07640553 -0.05259104]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.07640553]\n",
      " [-0.05259104]]\n"
     ]
    }
   ],
   "source": [
    "gradsU_2 = calculate_weights_gradients_U(y_true[1], y_pred[1], h=silu2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "Gradient of L w.r.t. U: [-0.03617086 -0.03168527]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.03617086]\n",
      " [-0.03168527]]\n"
     ]
    }
   ],
   "source": [
    "gradsU_3 = calculate_weights_gradients_U(y_true[2], y_pred[2], h=silu3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Matrix:\n",
      "[[-0.04997894]\n",
      " [ 0.00634858]]\n"
     ]
    }
   ],
   "source": [
    "stacked = np.stack((gradsU_1, gradsU_2, gradsU_3), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_u_mean = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_u_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_b3(y_true, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss with respect to b3 (second-layer bias).\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (float): Ground truth label.\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "    \n",
    "    Returns:\n",
    "        gradient_b3 (float): Gradient of the loss with respect to b3.\n",
    "    \"\"\"\n",
    "    # 1. Compute ∂L/∂y\n",
    "    dL_dy = -y_true / y + (1 - y_true) / (1 - y)\n",
    "    print(f\"dL/dy: {dL_dy}\")\n",
    "\n",
    "    # 2. Compute ∂y/∂f\n",
    "    dy_df = y * (1 - y)\n",
    "    print(f\"dy/df: {dy_df}\")\n",
    "\n",
    "    # 3. Chain rule: ∂L/∂b3\n",
    "    dL_db3 = dL_dy * dy_df\n",
    "    print(f\"Gradient of L w.r.t. b3: {dL_db3}\")\n",
    "\n",
    "    return dL_db3\n",
    "\n",
    "\n",
    "def calculate_bias_gradient_b_second(y_true, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss with respect to the bias b3 of the second layer.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (float): Ground truth label.\n",
    "        y (float): Output of the second layer (sigmoid activation).\n",
    "    \n",
    "    Returns:\n",
    "        gradient_b3 (float): Gradient of the loss with respect to b3.\n",
    "    \"\"\"\n",
    "    print(\"$$$ Computing Gradient for b3 (Second Layer Bias)\")\n",
    "    gradient_b3 = backward_pass_b3(y_true, y)\n",
    "\n",
    "    # Print the gradient\n",
    "    print(\"Gradient of the Loss w.r.t. b3 (Second Layer Bias):\")\n",
    "    print(gradient_b3)\n",
    "\n",
    "    return gradient_b3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: -1.8637441618760981\n",
      "dy/df: 0.24866378096087308\n",
      "Gradient of L w.r.t. b3: -0.46344567003586407\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "-0.46344567003586407\n"
     ]
    }
   ],
   "source": [
    "gradb3_1 = calculate_bias_gradient_b_second(y_true[0], y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: 2.0680127367070633\n",
      "dy/df: 0.2497295953902185\n",
      "Gradient of L w.r.t. b3: 0.5164439839996734\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "0.5164439839996734\n"
     ]
    }
   ],
   "source": [
    "gradb3_2 = calculate_bias_gradient_b_second(y_true[1], y_pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: 2.0890028238967604\n",
      "dy/df: 0.24954619473833461\n",
      "Gradient of L w.r.t. b3: 0.5213027055010719\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "0.5213027055010719\n"
     ]
    }
   ],
   "source": [
    "gradb3_3 = calculate_bias_gradient_b_second(y_true[2], y_pred[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Matrix:\n",
      "0.1914336731549604\n"
     ]
    }
   ],
   "source": [
    "stacked = np.stack((gradb3_1, gradb3_2, gradb3_3), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_b2_mean = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_b2_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 =  np.array([\n",
    "        [0.4, -0.3],  # w11, w12\n",
    "        [0.2,  0.2],  # w21, w22\n",
    "        [-0.5,  0.1]   # w31, w32\n",
    "    ])\n",
    "U0 = np.array([0.3, -0.1])\n",
    "b1_0 = np.array([0, -0.4])\n",
    "b2_0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(avg_grad_W, vt_W, W_old,name , gamma=0.9, eta=0.8):\n",
    "    \"\"\"\n",
    "    Updates the first-layer weights (W) using Gradient Descent with Momentum.\n",
    "    \n",
    "    Parameters:\n",
    "        avg_grad_W (2D array): Average gradients for W (shape: 3x2).\n",
    "        vt_W (2D array): Velocity for W (shape: 3x2).\n",
    "        W_old (2D array): Current weights W (shape: 3x2).\n",
    "        gamma (float): Momentum factor.\n",
    "        eta (float): Learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        W_new (2D array): Updated weights W (shape: 3x2).\n",
    "        vt_W_new (2D array): Updated velocity for W (shape: 3x2).\n",
    "    \"\"\"\n",
    "    # Compute new velocity\n",
    "    vt_W_new = gamma * vt_W - eta * avg_grad_W\n",
    "    print(f\"New Velocity for {name}:\\n{vt_W_new}\")\n",
    "    \n",
    "    # Update weights\n",
    "    W_new = W_old + vt_W_new\n",
    "    print(f\"Updated {name}:\\n{W_new}\")\n",
    "    \n",
    "    return W_new, vt_W_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for W:\n",
      "[[ 0.01997026 -0.02442525 -0.01513966]\n",
      " [-0.00189702  0.00870766  0.00833646]]\n",
      "Updated W:\n",
      "[[ 0.41997026  0.17557475 -0.51513966]\n",
      " [-0.30189702  0.20870766  0.10833646]]\n"
     ]
    }
   ],
   "source": [
    "W_1, vt_W_1 = update(grads_w_mean, np.zeros([2,3]), W0.T, name=\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for b(1):\n",
      "[[-0.00980641  0.00893704]]\n",
      "Updated b(1):\n",
      "[[-0.00980641 -0.39106296]]\n"
     ]
    }
   ],
   "source": [
    "b_1_1, vt_b11_1 = update(grads_b1_mean, np.zeros([1,2]), b1_0.T,name = 'b(1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for U:\n",
      "[[ 0.03998315 -0.00507887]]\n",
      "Updated U:\n",
      "[[ 0.33998315 -0.10507887]]\n"
     ]
    }
   ],
   "source": [
    "U_1, vt_u1_1 = update(grads_u_mean.T, np.zeros([1,2]), U0.T,name = 'U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for b(2):\n",
      "[[-0.15314694]]\n",
      "Updated b(2):\n",
      "[[-0.05314694]]\n"
     ]
    }
   ],
   "source": [
    "b_2_1, vt_b21_1 = update(grads_b2_mean.T, np.zeros([1,1]), b2_0,name = 'b(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05314694]]\n"
     ]
    }
   ],
   "source": [
    "print(b_2_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41997026,  0.17557475, -0.51513966],\n",
       "       [-0.30189702,  0.20870766,  0.10833646]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15259402 -0.63879175]]\n",
      "[[-0.39516167 -0.20856238]]\n",
      "[[-0.19482942 -0.10651979]]\n",
      "\n",
      "[[ 0.08210697 -0.22071515]]\n",
      "[[-0.15904283 -0.09344587]]\n",
      "[[-0.08795499 -0.05042596]]\n",
      "\n",
      "[[-0.00203945]]\n",
      "[[-0.09739964]]\n",
      "[[-0.07775145]]\n",
      "\n",
      "[[0.49949014]]\n",
      "[[0.47566932]]\n",
      "[[0.48057192]]\n"
     ]
    }
   ],
   "source": [
    "z1 = calculate_first_layer(X1,W_1.T,b_1_1)\n",
    "z2 = calculate_first_layer(X2,W_1.T,b_1_1)\n",
    "z3 = calculate_first_layer(X3,W_1.T,b_1_1)\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)\n",
    "print()\n",
    "silu1 = silu_activation(z1)\n",
    "silu2 = silu_activation(z2)\n",
    "silu3 = silu_activation(z3)\n",
    "print(silu1)\n",
    "print(silu2)\n",
    "print(silu3)\n",
    "print()\n",
    "f1 = calculate_second_layer(silu1,U_1.T,b_2_1)\n",
    "f2 = calculate_second_layer(silu2,U_1.T,b_2_1)\n",
    "f3 = calculate_second_layer(silu3,U_1.T,b_2_1)\n",
    "print(f1)\n",
    "print(f2)\n",
    "print(f3)\n",
    "print()\n",
    "y1 = sigmoid_activation(f1)\n",
    "y2 = sigmoid_activation(f2)\n",
    "y3 = sigmoid_activation(f3)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0])\n",
    "y_pred = np.array([y1, y2, y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6649423612035917\n",
      "[[[0.69416743 0.64563273 0.65502693]]]\n"
     ]
    }
   ],
   "source": [
    "def loss (y_true, y_pred):\n",
    "    individ_losses = -(y_true*(np.log(y_pred)) + (1-y_true)*(np.log(1-y_pred)))\n",
    "    return np.mean(individ_losses), individ_losses\n",
    "loss_iter2,individ_losses = loss(y_true, y_pred.T)\n",
    "print(loss_iter2)\n",
    "print(individ_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15259402, -0.63879175]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_1.flatten()\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$w_11 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.5760019446299496\n",
      "dz_0/dw_00: 1.0\n",
      "Gradient of L w.r.t. w_00: [[-0.09801533]]\n",
      "$$$w_12 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.5760019446299496\n",
      "dz_0/dw_01: 0.0\n",
      "Gradient of L w.r.t. w_01: [[-0.]]\n",
      "$$$w_13 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.5760019446299496\n",
      "dz_0/dw_02: 0.5\n",
      "Gradient of L w.r.t. w_02: [[-0.04900766]]\n",
      "$$$w_21 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.20106600496020557\n",
      "dz_1/dw_10: 1.0\n",
      "Gradient of L w.r.t. w_10: [[0.01057467]]\n",
      "$$$w_22 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.20106600496020557\n",
      "dz_1/dw_11: 0.0\n",
      "Gradient of L w.r.t. w_11: [[0.]]\n",
      "$$$w_23 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.20106600496020557\n",
      "dz_1/dw_12: 0.5\n",
      "Gradient of L w.r.t. w_12: [[0.00528733]]\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[-0.09801533 -0.         -0.04900766]\n",
      " [ 0.01057467  0.          0.00528733]]\n",
      "\n",
      "$$$w_11 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.307443342029572\n",
      "dz_0/dw_00: 0.1\n",
      "Gradient of L w.r.t. w_00: [[0.00497196]]\n",
      "$$$w_12 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.307443342029572\n",
      "dz_0/dw_01: 0.5\n",
      "Gradient of L w.r.t. w_01: [[0.0248598]]\n",
      "$$$w_13 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.307443342029572\n",
      "dz_0/dw_02: 1.0\n",
      "Gradient of L w.r.t. w_02: [[0.0497196]]\n",
      "$$$w_21 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.3964699148841661\n",
      "dz_1/dw_10: 0.1\n",
      "Gradient of L w.r.t. w_10: [[-0.00198167]]\n",
      "$$$w_22 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.3964699148841661\n",
      "dz_1/dw_11: 0.5\n",
      "Gradient of L w.r.t. w_11: [[-0.00990834]]\n",
      "$$$w_23 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.3964699148841661\n",
      "dz_1/dw_12: 1.0\n",
      "Gradient of L w.r.t. w_12: [[-0.01981667]]\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[ 0.00497196  0.0248598   0.0497196 ]\n",
      " [-0.00198167 -0.00990834 -0.01981667]]\n",
      "\n",
      "$$$w_11 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.4031980858623431\n",
      "dz_0/dw_00: 0.0\n",
      "Gradient of L w.r.t. w_00: [[0.]]\n",
      "$$$w_12 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.4031980858623431\n",
      "dz_0/dw_01: 1.0\n",
      "Gradient of L w.r.t. w_01: [[0.06587707]]\n",
      "$$$w_13 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.4031980858623431\n",
      "dz_0/dw_02: 0.7\n",
      "Gradient of L w.r.t. w_02: [[0.04611395]]\n",
      "$$$w_21 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.4468406545005932\n",
      "dz_1/dw_10: 0.0\n",
      "Gradient of L w.r.t. w_10: [[-0.]]\n",
      "$$$w_22 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.4468406545005932\n",
      "dz_1/dw_11: 1.0\n",
      "Gradient of L w.r.t. w_11: [[-0.02256454]]\n",
      "$$$w_23 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.4468406545005932\n",
      "dz_1/dw_12: 0.7\n",
      "Gradient of L w.r.t. w_12: [[-0.01579518]]\n",
      "Jacobian Matrix (∂L/∂W):\n",
      "[[ 0.          0.06587707  0.04611395]\n",
      " [-0.         -0.02256454 -0.01579518]]\n",
      "\n",
      "Mean Matrix:\n",
      "[[-0.03101446  0.03024562  0.01560863]\n",
      " [ 0.00286433 -0.01082429 -0.01010817]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouri\\AppData\\Local\\Temp\\ipykernel_16780\\2737743674.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  jacobian[i, j] = gradient  # Store the gradient in the Jacobian matrix\n"
     ]
    }
   ],
   "source": [
    "gradw1_2 = calculate_jacobian_dl_dW(x=X1, y_true=y_true[0], U=U_1.flatten(), z=z1.flatten(), y=y_pred[0])\n",
    "print()\n",
    "gradw2_2 = calculate_jacobian_dl_dW(x=X2, y_true=y_true[1], U=U_1.flatten(), z=z2.flatten(), y=y_pred[1])\n",
    "print()\n",
    "gradw3_2 = calculate_jacobian_dl_dW(x=X3, y_true=y_true[2], U=U_1.flatten(), z=z3.flatten(), y=y_pred[2])\n",
    "print()\n",
    "stacked = np.stack((gradw1_2, gradw2_2, gradw3_2), axis=0)\n",
    "grads_w_mean_2 = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_w_mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$b1 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.5760019446299496\n",
      "Gradient of L w.r.t. b_0: [[-0.09801533]]\n",
      "$$$b2 iteration\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.20106600496020557\n",
      "Gradient of L w.r.t. b_1: [[0.01057467]]\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[-0.09801533  0.01057467]\n",
      "$$$b1 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.307443342029572\n",
      "Gradient of L w.r.t. b_0: [[0.0497196]]\n",
      "$$$b2 iteration\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.3964699148841661\n",
      "Gradient of L w.r.t. b_1: [[-0.01981667]]\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[ 0.0497196  -0.01981667]\n",
      "$$$b1 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_0: 0.3399831534829202\n",
      "dh_0/dz_0: 0.4031980858623431\n",
      "Gradient of L w.r.t. b_0: [[0.06587707]]\n",
      "$$$b2 iteration\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "df/dh_1: -0.10507886799272378\n",
      "dh_1/dz_1: 0.4468406545005932\n",
      "Gradient of L w.r.t. b_1: [[-0.02256454]]\n",
      "Gradients of the Loss w.r.t. Biases [b1, b2]:\n",
      "[ 0.06587707 -0.02256454]\n",
      "\n",
      "Mean Matrix:\n",
      "[ 0.00586045 -0.01060218]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouri\\AppData\\Local\\Temp\\ipykernel_16780\\3491421506.py:59: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  gradients[i] = backward_pass_bi(i, y_true, U, z, y)\n"
     ]
    }
   ],
   "source": [
    "gradb1_1_2 = calculate_bias_gradients_b_first(y_true=y_true[0],U=U_1.flatten(), z=z1.flatten(), y=y_pred[0])\n",
    "gradb1_2_2 = calculate_bias_gradients_b_first(y_true=y_true[1],U=U_1.flatten(), z=z2.flatten(), y=y_pred[1])\n",
    "gradb1_3_2 = calculate_bias_gradients_b_first(y_true=y_true[2], U=U_1.flatten(), z=z3.flatten(), y=y_pred[2])\n",
    "print()\n",
    "stacked = np.stack((gradb1_1_2, gradb1_2_2, gradb1_3_2), axis=0)\n",
    "grads_b1_mean_2 = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_b1_mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "Gradient of L w.r.t. U: [[-0.04109535  0.11047011]]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.04109535]\n",
      " [ 0.11047011]]\n",
      "\n",
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "Gradient of L w.r.t. U: [[-0.0756518  -0.04444933]]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.0756518 ]\n",
      " [-0.04444933]]\n",
      "\n",
      "$$$ Computing Gradients for U (Second Layer Weights)\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "Gradient of L w.r.t. U: [[-0.0422687 -0.0242333]]\n",
      "Gradients of the Loss w.r.t. U (Second Layer Weights):\n",
      "[[-0.0422687]\n",
      " [-0.0242333]]\n",
      "\n",
      "Mean Matrix:\n",
      "[[-0.05300528]\n",
      " [ 0.01392916]]\n"
     ]
    }
   ],
   "source": [
    "gradsU_1_2 = calculate_weights_gradients_U(y_true[0], y_pred[0], h=silu1)\n",
    "print()\n",
    "gradsU_2_2 = calculate_weights_gradients_U(y_true[1], y_pred[1], h=silu2)\n",
    "print()\n",
    "gradsU_3_2 = calculate_weights_gradients_U(y_true[2], y_pred[2], h=silu3)\n",
    "print()\n",
    "stacked = np.stack((gradsU_1_2, gradsU_2_2, gradsU_3_2), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_u_mean_2 = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_u_mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: [[-2.00204153]]\n",
      "dy/df: [[0.24999974]]\n",
      "Gradient of L w.r.t. b3: [[-0.50050986]]\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "[[-0.50050986]]\n",
      "\n",
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: [[1.90719339]]\n",
      "dy/df: [[0.24940802]]\n",
      "Gradient of L w.r.t. b3: [[0.47566932]]\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "[[0.47566932]]\n",
      "\n",
      "$$$ Computing Gradient for b3 (Second Layer Bias)\n",
      "dL/dy: [[1.92519435]]\n",
      "dy/df: [[0.24962255]]\n",
      "Gradient of L w.r.t. b3: [[0.48057192]]\n",
      "Gradient of the Loss w.r.t. b3 (Second Layer Bias):\n",
      "[[0.48057192]]\n",
      "\n",
      "Mean Matrix:\n",
      "[[0.15191046]]\n"
     ]
    }
   ],
   "source": [
    "gradb3_1_2 = calculate_bias_gradient_b_second(y_true[0], y_pred[0])\n",
    "print()\n",
    "gradb3_2_2 = calculate_bias_gradient_b_second(y_true[1], y_pred[1])\n",
    "print()\n",
    "gradb3_3_2 = calculate_bias_gradient_b_second(y_true[2], y_pred[2])\n",
    "print()\n",
    "\n",
    "stacked = np.stack((gradb3_1_2, gradb3_2_2, gradb3_3_2), axis=0)\n",
    "\n",
    "# Calculate the mean\n",
    "grads_b2_mean_2 = np.mean(stacked, axis=0)\n",
    "\n",
    "print(\"Mean Matrix:\")\n",
    "print(grads_b2_mean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03101446,  0.03024562,  0.01560863],\n",
       "       [ 0.00286433, -0.01082429, -0.01010817]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_w_mean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for W:\n",
      "[[ 0.04278479 -0.04617922 -0.0261126 ]\n",
      " [-0.00399879  0.01649632  0.01558935]]\n",
      "Updated W:\n",
      "[[ 0.46275505  0.12939553 -0.54125227]\n",
      " [-0.30589581  0.22520398  0.1239258 ]]\n"
     ]
    }
   ],
   "source": [
    "W_2, vt_W_2 = update(grads_w_mean_2, vt_W_1, W_1, name=\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00980641, -0.39106296]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for b(1):\n",
      "[[-0.01351412  0.01652508]]\n",
      "Updated b(1):\n",
      "[[-0.02332053 -0.37453788]]\n"
     ]
    }
   ],
   "source": [
    "b_1_2, vt_b11_2 = update(grads_b1_mean_2,vt_b11_1 , b_1_1,name = 'b(1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05300528],\n",
       "       [ 0.01392916]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_u_mean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for U:\n",
      "[[ 0.07838906 -0.01571431]]\n",
      "Updated U:\n",
      "[[ 0.41837222 -0.12079318]]\n"
     ]
    }
   ],
   "source": [
    "U_2, vt_u1_2 = update(grads_u_mean_2.T, vt_u1_1, U_1,name = 'U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33998315, -0.10507887]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Velocity for b(2):\n",
      "[[-0.25936061]]\n",
      "Updated b(2):\n",
      "[[-0.31250755]]\n"
     ]
    }
   ],
   "source": [
    "b_2_2, vt_b21_2 = update(grads_b2_mean_2.T, vt_b21_1, b,name = 'b(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05314694]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
